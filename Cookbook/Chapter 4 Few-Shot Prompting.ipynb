{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 4. Using Examples and Few-Shot Prompting** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of well-known prompting techniques that use examples, such as zero-shot, one-shot, and few-shot prompting. By understanding each approach and the principles of designing effective examples, you can significantly enhance the performance of Solar Pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [zero-shot prompting](#zeroshot)\n",
    "- [one-shot prompting](#oneshot)\n",
    "- [few-shot prompting](#fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Retrieve the UPSTAGE_API_KEY variable from the IPython store\n",
    "%store -r UPSTAGE_API_KEY\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= UPSTAGE_API_KEY,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"zeroshot\"></a>\n",
    "### **4.1 Zero-shot prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique where the model is asked to perform a task without any prior examples. There are tons of ways to craft zero-shot prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Content Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"Classify the following text as positive, negative, or neutral. Text: I thought the macaron flavor was just okay. Sentiment: { }\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Practice Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Translate the term 'artificial tears' from English into Japanese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"<<[ Replace this text ]>>\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"oneshot\"></a>\n",
    "### **4.2 One-shot prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot prompting is a technique where the model is provided with only one example before it’s asked to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Content Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore étudier les langues.\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"Translate the following sentence into French: I like reading books.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\": \"J'aime lire des livres.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"Translate the following sentence into French: I love studying language\"\n",
    "    },\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Practice Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Translate the term 'artificial tears' from English to Japanese using one-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt:\n",
    "message = [{\"<<[ Replace this text ]>>\"}]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fewshot\"></a>\n",
    "### **4.3 Few-shot Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompting is a technique where the model is given a handful of examples—typically two to five—before it is asked to complete the task. This approach offers the model multiple references for the type of output desired, providing more context and making the output more accurate or in line with specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **It's important to note that few-shot learning and few-shot prompting are distinct concepts. Few-shot learning is a broader machine learning approach focused on adapting model parameters with only a few examples. In contrast, few-shot prompting specifically applies to prompt design in generative AI, where model parameters remain unchanged.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Content Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four hundred and one\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"2+10: twelve, 4+52: fifty-six, 100+301: \"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to highlight four ways to design effective examples when using Solar Pro. \n",
    "\n",
    "- [**exemplar quantity**](#quant)\n",
    "- [**exemplar similarity**](#sim)\n",
    "- [**exemplar format**](#format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quant\"></a>\n",
    "##### (1) Exemplar Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, the quantity of exemplars in the prompt generally improves model\n",
    "performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Thick smog chokes northern India and eastern Pakistan ahead of Diwali.\"\n",
      "Subject: \"Thick smog\"\n",
      "Verb: \"chokes\"\n",
      "Object: \"northern India and eastern Pakistan\"\n",
      "Adverbial Phrase: \"ahead of Diwali\"\n",
      "Analysis: Subject-verb-object-adverbial phrase structure, indicating the action is directed toward an object and providing context about the action's timing.\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"\n",
    "Sentence: \"The cat sleeps.\"\n",
    "Subject: \"The cat\"\n",
    "Verb: \"sleeps\"\n",
    "Object: None\n",
    "Analysis: Simple subject-verb sentence structure with a single actor performing an action.\n",
    "\n",
    "##\n",
    "Sentence: \"She reads books.\"\n",
    "Subject: \"She\"\n",
    "Verb: \"reads\"\n",
    "Object: \"books\"\n",
    "Analysis: Subject-verb-object structure, indicating the action is directed toward an object.\n",
    "\n",
    "## \n",
    "Sentence: \"They dance gracefully.\"\n",
    "Subject: \"They\"\n",
    "Verb: \"dance\"\n",
    "Adverb: \"gracefully\"\n",
    "Analysis: Subject-verb-adverb structure, with the adverb modifying how the action is performed.\n",
    "\n",
    "## \n",
    "Sentence: \"The sun rises in the east.\"\n",
    "Subject: \"The sun\"\n",
    "Verb: \"rises\"\n",
    "Prepositional Phrase: \"in the east\"\n",
    "Analysis: Subject-verb-prepositional phrase structure, adding context about the action's location.\n",
    "\n",
    "##\n",
    "setence: Thick smog chokes northern India and eastern Pakistan ahead of Diwali.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Thick smog\n",
      "Verb: chokes\n",
      "Object 1: northern India\n",
      "Object 2: eastern Pakistan\n",
      "Adverb: ahead of Diwali\n"
     ]
    }
   ],
   "source": [
    "# without an example\n",
    "\n",
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"Please analyze and define the sentence components in the following sentence.\n",
    "        sentence: \"Thick smog chokes northern India and eastern Pakistan ahead of Diwali.\"\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation: If you provide an example, you can see the model respond in the format used in the example. Even with complex and varied sentence structures, using many examples to guide the model helps it perform the task well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sim\"></a>\n",
    "##### (2) Exemplar Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select exemplars that are similar to your task. For example, if you are summarizing a news article, using exemplars in the same format will yield the best results you desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**content example**\n",
    "\n",
    "In the following prompt example, you can see that the summary results use **a consistent response format** as an example. By increasing the consistency of the format, you can achieve more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Open Source AI Definition / AI model with sufficient design information for substantial recreation, disclosed training data details./\n",
      "#OSAID Key Points / Freedom to use, modify, and build on top of open source AI models./\n",
      "#Challenges and Misuse / Mixed results in enforcing open source AI definition, with some companies not meeting criteria or misusing the term./\n"
     ]
    }
   ],
   "source": [
    "# When examples have the similarities\n",
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"Read the following text and summarize the key points. Once you finish the summary, organize it as follows:\n",
    "\n",
    "#Keyword 1 / Related key point in one short phrase./\n",
    "#Keyword 2 / Related key point in one short phrase./\n",
    "#Keyword 3 / Related key point in one short phrase./\n",
    "\n",
    "<text>\n",
    "To be considered open source under the OSAID, an AI model has to provide enough information about its design so that a person could “substantially” recreate it. The model must also disclose any pertinent details about its training data, including the provenance, how the data was processed, and how it can be obtained or licensed.\n",
    "“An open source AI is an AI model that allows you to fully understand how it’s been built,” Maffulli said. “That means that you have access to all the components, such as the complete code used for training and data filtering.”\n",
    "The OSAID also lays out usage rights developers should expect with open source AI, like the freedom to use the model for any purpose and modify it without having to ask anyone’s permission. “Most importantly, you should be able to build on top,” added Maffulli.\n",
    "The OSI has no enforcement mechanisms to speak of. It can’t pressure developers to abide by or follow the OSAID. But it does intend to flag models described as “open source” but which fall short of the definition.\n",
    "“Our hope is that when someone tries to abuse the term, the AI community will say, ‘We don’t recognize this as open source,’ and it gets corrected,” Maffulli said. Historically, this has had mixed results, but it isn’t entirely without effect.\n",
    "Many startups and big tech companies, most prominently Meta, have employed the term “open source” to describe their AI model release strategies — but few meet the OSAID’s criteria. For example, Meta mandates that platforms with over 700 million monthly active users request a special license to use its [Llama](https://techcrunch.com/2024/09/08/meta-llama-everything-you-need-to-know-about-the-open-generative-ai-model/) models.\n",
    "Maffulli has been [openly critical](https://www.ft.com/content/397c50d8-8796-4042-a814-0ac2c068361f) of Meta’s decision to call its models “open source.” After discussions with the OSI, Google and Microsoft agreed to drop their use of the term for models that aren’t fully open, but Meta hasn’t, he said.\n",
    "Stability AI, which has long advertised its models as “open,” requires that businesses making more than $1 million in revenue obtain an enterprise license. And French AI upstart Mistral’s license bars the use of certain models and outputs for commercial ventures.\n",
    "A [study](https://www.wired.com/story/the-myth-of-open-source-ai/) last August by researchers at the Signal Foundation, the nonprofit AI Now Institute, and Carnegie Mellon found that many “open source” models are basically open source in name only. The data required to train the models is kept secret, the compute power needed to run them is beyond the reach of many developers, and the techniques to fine-tune them are intimidatingly complex.\n",
    "Instead of democratizing AI, these “open source” projects tend to entrench and expand centralized power, the study’s authors concluded. Indeed, Meta’s Lllama models have [racked up](https://venturebeat.com/ai/meta-leads-open-source-ai-boom-llama-downloads-surge-10x-year-over-year/) hundreds of millions of downloads, and Stability [claims](https://www.prnewswire.com/news-releases/stability-ai-secures-significant-new-investment-from-world-class-investor-group-and-appoints-prem-akkaraju-as-ceo-302181923.html) that its models power up to 80% of all AI-generated imagery.\n",
    "</text>\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When examples don't have the similarities\n",
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"Read the following text and summarize the key points. Once you finish the summary, organize it as follows:\n",
    "\n",
    "Related key point in one short phrase./Keyword 1\n",
    "Keyword 2 Related key point in two short phrases.\n",
    "Keyword 3/ Related key point in three short phrases.\n",
    "\n",
    "<text>\n",
    "To be considered open source under the OSAID, an AI model has to provide enough information about its design so that a person could “substantially” recreate it. The model must also disclose any pertinent details about its training data, including the provenance, how the data was processed, and how it can be obtained or licensed.\n",
    "“An open source AI is an AI model that allows you to fully understand how it’s been built,” Maffulli said. “That means that you have access to all the components, such as the complete code used for training and data filtering.”\n",
    "The OSAID also lays out usage rights developers should expect with open source AI, like the freedom to use the model for any purpose and modify it without having to ask anyone’s permission. “Most importantly, you should be able to build on top,” added Maffulli.\n",
    "The OSI has no enforcement mechanisms to speak of. It can’t pressure developers to abide by or follow the OSAID. But it does intend to flag models described as “open source” but which fall short of the definition.\n",
    "“Our hope is that when someone tries to abuse the term, the AI community will say, ‘We don’t recognize this as open source,’ and it gets corrected,” Maffulli said. Historically, this has had mixed results, but it isn’t entirely without effect.\n",
    "Many startups and big tech companies, most prominently Meta, have employed the term “open source” to describe their AI model release strategies — but few meet the OSAID’s criteria. For example, Meta mandates that platforms with over 700 million monthly active users request a special license to use its [Llama](https://techcrunch.com/2024/09/08/meta-llama-everything-you-need-to-know-about-the-open-generative-ai-model/) models.\n",
    "Maffulli has been [openly critical](https://www.ft.com/content/397c50d8-8796-4042-a814-0ac2c068361f) of Meta’s decision to call its models “open source.” After discussions with the OSI, Google and Microsoft agreed to drop their use of the term for models that aren’t fully open, but Meta hasn’t, he said.\n",
    "Stability AI, which has long advertised its models as “open,” requires that businesses making more than $1 million in revenue obtain an enterprise license. And French AI upstart Mistral’s license bars the use of certain models and outputs for commercial ventures.\n",
    "A [study](https://www.wired.com/story/the-myth-of-open-source-ai/) last August by researchers at the Signal Foundation, the nonprofit AI Now Institute, and Carnegie Mellon found that many “open source” models are basically open source in name only. The data required to train the models is kept secret, the compute power needed to run them is beyond the reach of many developers, and the techniques to fine-tune them are intimidatingly complex.\n",
    "Instead of democratizing AI, these “open source” projects tend to entrench and expand centralized power, the study’s authors concluded. Indeed, Meta’s Lllama models have [racked up](https://venturebeat.com/ai/meta-leads-open-source-ai-boom-llama-downloads-surge-10x-year-over-year/) hundreds of millions of downloads, and Stability [claims](https://www.prnewswire.com/news-releases/stability-ai-secures-significant-new-investment-from-world-class-investor-group-and-appoints-prem-akkaraju-as-ceo-302181923.html) that its models power up to 80% of all AI-generated imagery.\n",
    "\n",
    "</text>\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "responses = []\n",
    "for i in range(3):\n",
    "    response = client.chat.completions.create(**config)\n",
    "    responses.append(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As seen in the following results, prompts with lower similarity among examples yield a wider variety of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1\n",
      "Keyword 1: Open Source AI Definition\n",
      "Open source AI requires detailed design and training data information.\n",
      "\n",
      "Keyword 2: Usage Rights and Community Enforcement\n",
      "Open source AI allows full usage, modification, and building upon; community enforcement relies on recognition.\n",
      "\n",
      "Keyword 3: Industry Practices and Criticism\n",
      "Many companies' AI models, despite being labeled \"open source,\" do not meet the definition, leading to centralized power and criticism from the AI community.\n",
      "\n",
      "\n",
      "n = 2\n",
      "Key point: Open source AI definition\n",
      "OSAID defines open source AI as a model with accessible design and training data information.\n",
      "\n",
      "Keyword 1: Open source AI\n",
      "Related key point: Full understanding and usage rights\n",
      "An open source AI allows full understanding of its construction and grants freedom to use, modify, and build upon it.\n",
      "\n",
      "Keyword 2: OSAID enforcement\n",
      "Related key point: Community pressure and mixed results\n",
      "The OSI has no enforcement mechanisms but relies on community pressure to correct misuse of the \"open source\" term, with historically mixed results.\n",
      "\n",
      "Keyword 3: Industry practices\n",
      "Related key point: Limited adherence to OSAID criteria\n",
      "Many companies, like Meta, use \"open source\" to describe AI models that don't meet OSAID criteria, leading to centralized power instead of democratization.\n",
      "\n",
      "\n",
      "n = 3\n",
      "Open Source AI Definition: AI model with detailed design, training data info, and usage rights.\n",
      "\n",
      "\"Substantially\" recreate: AI model provides enough information about its design.\n",
      "\n",
      "Training data details: Model discloses provenance, processing, and obtaining/licensing of training data.\n",
      "\n",
      "Usage rights: Freedom to use, modify, and build on the model without permission.\n",
      "\n",
      "OSI's role: No enforcement, but flags non-compliant models, hoping for community correction.\n",
      "\n",
      "Mixed results: Historically, community correction has had mixed results, but it's not entirely ineffective.\n",
      "\n",
      "Meta's Llama: Meta's use of \"open source\" for Llama models, which require special licensing for large platforms, has been criticized.\n",
      "\n",
      "Other companies: Google, Microsoft have dropped \"open source\" for non-compliant models, while Stability AI and Mistral have restrictions on their models.\n",
      "\n",
      "Study findings: Many \"open source\" models are not truly open, entrenching centralized power instead of democratizing AI.\n",
      "\n",
      "Meta and Stability AI dominance: Meta's Llama models have millions of downloads, and Stability claims its models power up to 80% of AI-generated imagery.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"n = {i+1}\")\n",
    "    print(responses[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"format\"></a>\n",
    "##### (3) Exemplar Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a common template. Optimal format may vary across tasks. \n",
    "\n",
    "There is evidence indicating that formats frequently found in the training data tend to result in improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company names:\n",
      "- SOLAR\n",
      "\n",
      "personal names:\n",
      "- None\n",
      "\n",
      "specific topics:\n",
      "- large language models (LLMs)\n",
      "- depth up-scaling (DUS)\n",
      "- mixture-of-experts\n",
      "- Mixtral-8x7B-Instruct\n",
      "- Apache 2.0 license\n",
      "\n",
      "themes:\n",
      "- natural language processing (NLP) tasks\n",
      "- efficient up-scaling of LLMs\n",
      "- depthwise scaling\n",
      "- continued pretraining\n",
      "- instruction-following capabilities\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"Identify the entities referenced in the text below. \n",
    "\n",
    "Please extract the following four types of entities:\n",
    "- company names: \n",
    "- personal names: \n",
    "- specific topics:\n",
    "- themes:\n",
    "\n",
    "<text>\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.\n",
    "</text>\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When not using a standard format\n",
    "# Prompt:\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"\"\"Identify the entities referenced in the text below. \n",
    "\n",
    "Please extract the following four types of entities:\n",
    "% % company names\n",
    "% % personal names \n",
    "% % specific topics% % \n",
    "% %  themes % % \n",
    "\n",
    "~text~\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.\n",
    "~text~\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "responses = []\n",
    "for i in range(2):\n",
    "    response = client.chat.completions.create(**config)\n",
    "    responses.append(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When using unusual format symbols or marks, the stability of the results decreases each time they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1\n",
      "company names:\n",
      "* (none)\n",
      "\n",
      "personal names:\n",
      "* (none)\n",
      "\n",
      "specific topics:\n",
      "* large language models (LLM)\n",
      "* natural language processing (NLP) tasks\n",
      "* depth up-scaling (DUS)\n",
      "* depthwise scaling\n",
      "* continued pretraining\n",
      "* mixture-of-experts\n",
      "* SOLAR 10.7B\n",
      "* SOLAR 10.7B-Instruct\n",
      "* Mixtral-8x7B-Instruct\n",
      "\n",
      "themes:\n",
      "* scaling up LLMs\n",
      "* instruction-following capabilities\n",
      "* public availability of LLMs\n",
      "\n",
      "\n",
      "n = 2\n",
      "company names:\n",
      "* SOLAR (referring to the company that developed SOLAR 10.7B)\n",
      "\n",
      "personal names:\n",
      "* None\n",
      "\n",
      "specific topics:\n",
      "* large language model (LLM)\n",
      "* natural language processing (NLP) tasks\n",
      "* depth up-scaling (DUS)\n",
      "* depthwise scaling\n",
      "* continued pretraining\n",
      "* mixture-of-experts\n",
      "* instruction-following capabilities\n",
      "* Apache 2.0 license\n",
      "\n",
      "themes:\n",
      "* scaling up large language models\n",
      "* improving performance in natural language processing tasks\n",
      "* efficient training and inference methods\n",
      "* promoting broad access and application in the LLM field\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(f\"n = {i+1}\")\n",
    "    print(responses[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Practice Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well Done!**\n",
    "\n",
    "Having completed all the exercises, you're now ready to proceed to the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
