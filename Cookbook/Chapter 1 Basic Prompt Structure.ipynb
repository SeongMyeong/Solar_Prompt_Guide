{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 1. Basic Prompt Structure** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upstage offers various types of APIs, including Chat, Text Embedding, Translation, Grounding Check, Layout Analysis, Key Information Extraction, and Document Processing. In this book, we will exclusively focus on using the `Chat API`.\n",
    "\n",
    "For more information about the APIs, please refer to the following [link](https://github.com/UpstageAI/cookbook?tab=readme-ov-file#api-list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 1.1 Chat API ](#section1)\n",
    "- [ 1.2 Understanding Parameters ](#section2)\n",
    "- [ 1.3 Understanding Structure ](#section3)\n",
    "- [ 1.4 Understanding System Prompt ](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "### **1.1 `Chat API`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a standard API call format used to interact with Upstage’s API for generating chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve the UPSTAGE_API_KEY variable from the IPython store\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r UPSTAGE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mUPSTAGE_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.upstage.ai/v1/solar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolar-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Retrieve the UPSTAGE_API_KEY variable from the IPython store\n",
    "%store -r UPSTAGE_API_KEY\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= UPSTAGE_API_KEY,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"solar-pro\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe how we plan to leverage Upstage products to achieve your mission of AGI for Work.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'UPSTAGE_API_KEY=your-api-key-here'\n"
     ]
    }
   ],
   "source": [
    "# Store your API key (run this first)\n",
    "%store UPSTAGE_API_KEY=\"\"\n",
    "\n",
    "# Then your existing code will work\n",
    "client = OpenAI(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### **1.2 Understanding Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing prompt engineering, parameters are key to controlling how the model behaves and the type of output you receive. <br>\n",
    "Here’s a detailed explanation of these parameters and their role in the completion generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [model](#model)\n",
    "- [max_tokens](#maxtoken)\n",
    "- [temperature](#temp)\n",
    "- [Top_P](#topp)\n",
    "\n",
    "[summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "**`Model`**:  `Solar_pro`\n",
    "\n",
    "The specific model you are intending to interact with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"maxtoken\"></a>\n",
    "**`Max_Tokens`**: \n",
    "\n",
    "- This parameter limits the total number of tokens (words or parts of words) in the output. Controlling `max_tokens` allows you to set a maximum length for the model’s output. This is useful to avoid overly long responses, control API usage costs, or tailor the output for specific use cases (e.g., short answers, summaries, etc.).\n",
    "\n",
    "- **Hard** **stop :**\n",
    "    - Prevents the model from generating tokens beyond the specific limit.\n",
    "    - The generation may stop mid-word or mid-sentence when the token limit is reached.\n",
    "\n",
    "- **Prompt tokens** : The number of tokens in the input prompt.\n",
    "\n",
    "- If `max_tokens` is set, the sum of input tokens and max_tokens must less than or equal to the model’s  context length (≤ 4096 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"temp\"></a>\n",
    "**`Temperature`**\n",
    "\n",
    "This parameter controls the randomness or creativity of the model’s responses. \n",
    "\n",
    "- A higher value allows for more flexibility, resulting in more diverse text generation.\n",
    "- A lower value makes the model more deterministic, typically generating more accurate and consistent output.\n",
    "\n",
    "The valid range is between **0** and **2.0** (`0 ≤ Temperature ≤ 2.0`).\n",
    "\n",
    "- `0.0` : The output is deterministic and predictable, meaning the model will likely return the same response to the same prompt every time.\n",
    "- **`0.7`**: This is a balanced level, where the model is creative but still focused. The responses may vary, but they tend to stay on topic.\n",
    "- **`2.0`**: This encourages highly creative or random output, potentially producing more unusual or diverse responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topp\"></a>\n",
    "**`Top_P`** \n",
    "\n",
    " This is an alternative way to control the randomness of the model's output by considering the cumulative probability of token choices. `Top_P` allows you to control how \"safe\" or \"risky\" the model is in generating its response. Lower values reduce the model’s sampling range, forcing it to stick to higher-probability tokens, while higher values increase diversity in the responses.\n",
    "\n",
    "- **Top_P = 0.9** means the model will sample tokens from the smallest set whose cumulative probability is 90%.\n",
    "\n",
    "**! How it differs from `temperature`**: While `temperature` affects how creative the model is overall, `Top_P` affects how many of the high-probability tokens are considered in the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "**Summary** \n",
    "\n",
    "- **model**: Defines the specific AI model being used.\n",
    "- **max_tokens**: Limits the length of the response.\n",
    "- **temperature**: Controls the creativity or randomness of the response.\n",
    "- **Top_P**: Controls how many token choices the model considers based on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Content Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #1: Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 150,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #2: Temperature and Top_P Adjustment**\n",
    "\n",
    "Objective:  Compare how creativity and randomness affect responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI in healthcare can bring numerous benefits, such as improved diagnostics, personalized treatment plans, and enhanced patient care. It can also help in drug discovery, reducing medical errors, and managing healthcare costs. Additionally, AI can assist in remote patient monitoring and provide valuable insights through data analysis.\n"
     ]
    }
   ],
   "source": [
    "config_robust = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the potential benefits of AI in healthcare?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 1.0\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_robust)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some key benefits of AI in healthcare: accurate diagnosis through imaging, predictive clinical supports, virtual assistants for hospital staff and streaming consulting for patients around the world, mentioning just a few. These technologies can optimize care resource access, precision, and personalization.\n"
     ]
    }
   ],
   "source": [
    "config_creative = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the potential benefits of AI in healthcare?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 2.0,\n",
    "    \"top_p\": 0.8\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_creative)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #3: Limiting Output with max_tokens**\n",
    "\n",
    "Objective: Control the length of responses and stop them at specific points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Upstage AI models handle natural language processing (NLP) by using advanced machine learning techniques. Think of it like teaching a robot to understand and respond to human language.\n",
      "\n",
      "First, we feed our models with a large amount of text data, like books, articles, and conversations. This helps the models learn the patterns, structures, and meanings of different languages.\n",
      "\n",
      "Next, when you ask a question or give a command, the model analyzes your words, identifies the key elements, and interprets their meaning. It then generates a response that's relevant and accurate, based on what it learned during training.\n",
      "\n",
      "In simple terms, Upstage AI models learn from lots of text and use that knowledge to understand and respond to human language in a smart and helpful way.\n"
     ]
    }
   ],
   "source": [
    "config_output_400 = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how Upstage AI models handle natural language processing. Explain it in a way that non-developers can easily understand.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 400,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_output_400)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstage AI models handle natural language processing by first breaking down sentences into smaller pieces, like words and phrases. Then, they analyze these pieces to understand their meanings and relationships with each other\n"
     ]
    }
   ],
   "source": [
    "config_output_40 = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how Upstage AI models handle natural language processing. Explain it in a way that non-developers can easily understand.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 40,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_output_40)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "### **1.3 Understanding Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**messages: system, user, assistant**](#message)\n",
    "\n",
    "- [**content example**](#contentex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"message\"></a>\n",
    "**`messages`** : \n",
    "\n",
    "It is an array containing the conversation context. It includes exchanged between the user and the model. Each contains: \n",
    "\n",
    "- “role”:\n",
    "    \n",
    "    The role can be `\"user\"`, `\"assistant\"`, or `\"system\"`, indicating the source of the message.\n",
    "    \n",
    "    In the case of `\"role\": \"system\"`, it sets the behavior, tone, and knowledge base of the assistant, acting as an initial instruction.\n",
    "    \n",
    "    In the case of `\"role\": \"user\"`, it specifies that the message comes from the user.\n",
    "    \n",
    "    In the case of `\"role\": \"assistant\"`, it contains responses generated by the AI to address the user’s queries or continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contentex\"></a>\n",
    "#### **Content Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are my Assistant. Your role is to answer my questions faithfully and in detail. \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Hello, Solar. Can you help me plan a weekend trip to New York City?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"assistnat\",\n",
    "  \"content\": \"Hello! I'd be happy to help you plan your weekend trip to New York City. Let's start by discussing your interests and preferences. Are you looking for sightseeing, shopping, diningor perhaps a mix of all?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### **1.4 Understanding System Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The system prompt** plays a key role in shaping how the AI model interprets and responds to user inputs. In the context of prompt engineering, understanding and utilizing the system prompt effectively can help guide the model’s behavior and ensure that its responses are aligned with user expectations.  In this book, we shows three different types of system prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tips**:\n",
    ">\n",
    "> If the system prompt is short, the responses tend to be short as well, and if the system prompt is long, the responses tend to be longer. The Solar Pro Preview model also shows a linear increase in response quantity based on the number of tokens used in the system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** : \n",
    "If you’re curious about the conceptual difference between a *system prompt* and a *user prompt*, read the following definition.\n",
    "- System instructions provide additional high-levle guidlines that an LLM must follow when responding to individual user prompts. These instructions are distinguished by a \"system\" role flag within a ChatML dialogue interface. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Content Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #1: Short Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a decentralized, distributed digital ledger that records transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This technology allows for secure, transparent, and tamper-proof record-keeping. It's most commonly associated with cryptocurrencies like Bitcoin, but it has many other potential applications, such as in supply chain management, voting systems, and digital identity verification.\n"
     ]
    }
   ],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant to help user's various tasks. Please provide me with an accurate information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain about Blockchain in detail.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #2: Long Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a decentralized, distributed digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This technology was first introduced in 2008 with the creation of Bitcoin, a cryptocurrency.\n",
      "\n",
      "A blockchain is composed of a series of blocks, each containing a list of transactions. Each block is linked to the previous one through a cryptographic hash, creating a chain of blocks. This structure ensures the integrity and security of the data, as any change in a block would require changing all subsequent blocks, which is practically impossible due to the distributed nature of the network.\n",
      "\n",
      "Blockchain technology has many potential applications beyond cryptocurrencies, such as supply chain management, voting systems, and digital identity verification. Its key features include decentralization, transparency, immutability, and security.\n",
      "\n",
      "If you need more specific information or have any questions about blockchain, please let me know!\n"
     ]
    }
   ],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your name is Solar. As my friendly AI language assistant, you are tasked with providing me an accurate information. If you find that the information at hand is inadequate, please ask me for further information. [Strong Rule] If you don't have any real-time information about the user’s query, please be honesty.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain about Blockchain in detail.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Practice Exercises**\n",
    "\n",
    "Try switching between the short and long versions of the system prompt with different questions to experience the difference in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant to help user's various tasks. Please provide me with an accurate information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<<[ Replace this text ]>>\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your name is Solar. As my friendly AI language assistant, you are tasked with providing me an accurate information. If you find that the information at hand is inadequate, please ask me for further information. [Strong Rule] If you don't have any real-time information about the user’s query, please be honesty.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<<[ Replace this text ]>>\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Your System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<<[ Replace this text ]>>\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<<[ Replace this text ]>>\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well Done!**\n",
    "\n",
    "Having completed all the exercises, you're now ready to proceed to the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
