{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 1. Basic Prompt Structure** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upstage offers various types of APIs, including Chat, Text Embedding, Translation, Grounding Check, Layout Analysis, Key Information Extraction, and Document Processing. In this book, we will exclusively focus on using the `Chat API`.\n",
    "\n",
    "For more information about the APIs, please refer to the following [link](https://github.com/UpstageAI/cookbook?tab=readme-ov-file#api-list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 1.1 Chat API ](#section1)\n",
    "- [ 1.2 Understanding Parameters ](#section2)\n",
    "- [ 1.3 Understanding Structure ](#section3)\n",
    "- [ 1.4 Understanding System Prompt ](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## **1.1 `Chat API`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a standard API call format used to interact with Upstage’s API for generating chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Upstage, we plan to leverage our AI solutions, specifically Solar LLM and Document AI, to achieve our mission of AGI for Work. Solar LLM, our advanced Language Learning Model, will help us build highly intelligent and adaptive language-based AI systems. Meanwhile, Document AI will enable our AI to understand, extract, and analyze information from documents, providing a comprehensive understanding of various data sources. By combining these core technologies and integrating them with other AI and ML solutions, we aim to create a powerful platform that can perform complex tasks, learn from experience, and collaborate with humans, ultimately leading us closer to AGI for Work.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Retrieve the UPSTAGE_API_KEY variable from the IPython store\n",
    "%store -r UPSTAGE_API_KEY\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= UPSTAGE_API_KEY,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"solar-pro\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe how we plan to leverage Upstage products to achieve your mission of AGI for Work.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## **1.2 Understanding Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing prompt engineering, parameters are key to controlling how the model behaves and the type of output you receive. <br>\n",
    "Here’s a detailed explanation of these parameters and their role in the completion generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [model](#model)\n",
    "- [max_tokens](#maxtoken)\n",
    "- [temperature](#temp)\n",
    "- [Top_P](#topp)\n",
    "\n",
    "[summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.1 Paramters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "**`Model`**:\n",
    "\n",
    "The specific model you are intending to interact with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"maxtoken\"></a>\n",
    "**`Max_Tokens`**: \n",
    "\n",
    "- This parameter limits the total number of tokens (words or parts of words) in the output. Controlling `max_tokens` allows you to set a maximum length for the model’s output. This is useful to avoid overly long responses, control API usage costs, or tailor the output for specific use cases (e.g., short answers, summaries, etc.).\n",
    "\n",
    "- **Hard Stop**:\n",
    "    - Prevents the model from generating tokens beyond the specific limit.\n",
    "    - The generation may stop mid-word or mid-sentence when the token limit is reached.\n",
    "\n",
    "- **Prompt Tokens**: The number of tokens in the input prompt.\n",
    "\n",
    "- If `max_tokens` is set, the sum of input tokens and max_tokens must less than or equal to the model’s  context length (≤ 4096 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"temp\"></a>\n",
    "**`Temperature`**:\n",
    "\n",
    "This parameter controls the randomness or creativity of the model’s responses. \n",
    "\n",
    "- A higher value allows for more flexibility, resulting in more diverse text generation.\n",
    "- A lower value makes the model more deterministic, typically generating more accurate and consistent output.\n",
    "\n",
    "The valid range is between **0** and **2.0** (`0 ≤ Temperature ≤ 2.0`).\n",
    "\n",
    "- **`0.0`**: The output is deterministic and predictable, meaning the model will likely return the same response to the same prompt every time.\n",
    "- **`0.7`**: This is a balanced level, where the model is creative but still focused. The responses may vary, but they tend to stay on topic.\n",
    "- **`2.0`**: This encourages highly creative or random output, potentially producing more unusual or diverse responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topp\"></a>\n",
    "**`Top_P`**: \n",
    "\n",
    " This is an alternative way to control the randomness of the model's output by considering the cumulative probability of token choices. `Top_P` allows you to control how \"safe\" or \"risky\" the model is in generating its response. Lower values reduce the model’s sampling range, forcing it to stick to higher-probability tokens, while higher values increase diversity in the responses.\n",
    "\n",
    "- **Top_P = 0.9** means the model will sample tokens from the smallest set whose cumulative probability is 90%.\n",
    "\n",
    "**! How it differs from `temperature`**: While `temperature` affects how creative the model is overall, `Top_P` affects how many of the high-probability tokens are considered in the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "> **Summary** \n",
    "> \n",
    "> - **model**: Defines the specific AI model being used.\n",
    "> - **max_tokens**: Limits the length of the response.\n",
    "> - **temperature**: Controls the creativity or randomness of the response.\n",
    "> - **top_p**: Controls how many token choices the model considers based on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.2 Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #1: Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #2: Temperature and Top_P Adjustment**\n",
    "\n",
    "Objective:  Compare how creativity and randomness affect responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI in healthcare can bring numerous benefits, such as improved patient outcomes, increased efficiency, and reduced costs. It can help with early disease detection, personalized treatment plans, and drug discovery. AI can also assist in administrative tasks, like scheduling appointments and managing patient records. Additionally, it can support mental health through chatbots and virtual therapists.\n"
     ]
    }
   ],
   "source": [
    "config_robust = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the potential benefits of AI in healthcare?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 1.0\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_robust)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great question! AI in healthcare can potentially offer numerous benefits. For example, AI-powered systems can aid in early and accurate disease detection, analyze large datasets to assist in medical research, provide personalized patient care based on medical history, automate administrative tasks to save medical professionals' time, and even enhance drug discovery and development processes.\n"
     ]
    }
   ],
   "source": [
    "config_creative = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the potential benefits of AI in healthcare?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_creative)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #3: Limiting Output with Max_Tokens**\n",
    "\n",
    "Objective: Control the length of responses and stop them at specific points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Upstage AI models handle natural language processing by first breaking down sentences into smaller parts like words or phrases. Then, they analyze these parts to understand their meaning, context, and relationship with other parts in the sentence. This process helps the AI model grasp the overall message or intent behind the text.\n",
      "\n",
      "Next, Upstage's models use advanced machine learning techniques to learn from vast amounts of data, enabling them to improve their understanding of language over time. This way, they can recognize different writing styles, dialects, and even slang.\n",
      "\n",
      "Lastly, these models can generate responses or take actions based on their understanding of the input text. This ability makes them useful for tasks like language translation, content creation, and customer support.\n"
     ]
    }
   ],
   "source": [
    "config_output_400 = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how Upstage AI models handle natural language processing. Explain it in a way that non-developers can easily understand.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 400,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_output_400)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstage's AI models handle natural language processing (NLP) in a way that's similar to how humans understand and respond to language. They use advanced machine learning techniques to analyze and\n"
     ]
    }
   ],
   "source": [
    "config_output_40 = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how Upstage AI models handle natural language processing. Explain it in a way that non-developers can easily understand.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 40,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**config_output_40)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## **1.3 Understanding Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3.1 Input Structure** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`messages`** :\n",
    "\n",
    "It is an array containing the conversation context. It includes exchanged between the user and the model. Each contains: \n",
    "\n",
    "- “**role**”:\n",
    "    \n",
    "    The role can be `\"user\"`, `\"assistant\"`, or `\"system\"`, indicating the source of the message.\n",
    "    \n",
    "    In the case of `\"role\": \"system\"`, it sets the behavior, tone, and knowledge base of the assistant, acting as an initial instruction.\n",
    "    \n",
    "    In the case of `\"role\": \"user\"`, it specifies that the message comes from the user.\n",
    "    \n",
    "    In the case of `\"role\": \"assistant\"`, it contains responses generated by the AI to address the user’s queries or continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3.2 Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are my Assistant. Your role is to answer my questions faithfully and in detail.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Hello, Solar. Can you help me plan a weekend trip to New York City?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"role\": \"assistnat\",\n",
    "  \"content\": \"Hello! I'd be happy to help you plan your weekend trip to New York City. Let's start by discussing your interests and preferences. Are you looking for sightseeing, shopping, diningor perhaps a mix of all?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## **1.4 Understanding System Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4.1 System Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The system prompt** plays a key role in shaping how the AI model interprets and responds to user inputs. In the context of prompt engineering, understanding and utilizing the system prompt effectively can help guide the model’s behavior and ensure that its responses are aligned with user expectations.  In this book, we show **two different types of system prompts: a short version and a long version.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tips**:\n",
    ">\n",
    "> If the system prompt is short, the responses tend to be brief; if the system prompt is long, the responses are generally longer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4.2 Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #1: Short Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a decentralized, distributed, and public digital ledger that records transactions across many computers in a way that makes it difficult or impossible to change, hack, or cheat the system. Here are some key features of blockchain:\n",
      "\n",
      "1. Decentralized: Blockchain doesn't have a central authority or administrator, making it less vulnerable to fraud or hacking.\n",
      "\n",
      "2. Transparent: The records in the blockchain are visible to everyone in the network, ensuring transparency and trust.\n",
      "\n",
      "3. Immutable: Once data is recorded in the blockchain, it can't be changed or deleted, ensuring data integrity and security.\n",
      "\n",
      "4. Distributed: Blockchain is maintained by a network of computers, ensuring data availability and resilience.\n",
      "\n",
      "5. Cryptography: Blockchain uses advanced cryptographic techniques to secure transactions and protect data.\n",
      "\n",
      "6. Smart Contracts: Some blockchains, like Ethereum, support smart contracts, which are self-executing contracts with the terms of the agreement directly written into code.\n",
      "\n",
      "Blockchain is best known for its use in cryptocurrencies like Bitcoin, but it has many other potential applications, such as supply chain management, voting systems, digital identity, and more.\n"
     ]
    }
   ],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant to help user's various tasks. Please provide me with an accurate information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain about Blockchain in detail.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example #2: Long Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a decentralized, distributed digital ledger that records transactions across multiple computers in a way that ensures security, transparency, and immutability. It was originally designed to support the digital currency, Bitcoin, but has since evolved to be used in various industries.\n",
      "\n",
      "A blockchain consists of a chain of blocks, where each block contains a list of transactions. These transactions are verified by a network of computers, known as nodes, using cryptographic algorithms. Once a block is filled with transactions, it is added to the chain, and the information in it becomes permanent and publicly accessible.\n",
      "\n",
      "Key features of blockchain include:\n",
      "\n",
      "1. Decentralization: Instead of relying on a central authority, blockchain uses a peer-to-peer network of nodes to validate transactions.\n",
      "2. Transparency: All transactions are visible to anyone with access to the network, promoting trust and accountability.\n",
      "3. Immutability: Once data is added to the blockchain, it cannot be altered or deleted, ensuring data integrity.\n",
      "4. Security: Blockchain uses advanced cryptographic techniques to secure data and prevent unauthorized access or tampering.\n",
      "\n",
      "Blockchain has potential applications in various sectors, including finance, supply chain management, healthcare, voting systems, and more. However, it also faces challenges such as scalability, energy consumption, and regulatory issues.\n",
      "\n",
      "For more specific information, please let me know what aspect of blockchain you are interested in.\n"
     ]
    }
   ],
   "source": [
    "config_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your name is Solar. As my friendly AI language assistant, you are tasked with providing me an accurate information. If you find that the information at hand is inadequate, please ask me for further information. [Strong Rule] If you don't have any real-time information about the user’s query, please be honesty.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain about Blockchain in detail.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4.3 Practice**\n",
    "\n",
    "Try switching between the short and long versions of the system prompt with different questions to experience the difference in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant to help user's various tasks. Please provide me with an accurate information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \" \" # ←- Insert your prompt here.\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your name is Solar. As my friendly AI language assistant, you are tasked with providing me an accurate information. If you find that the information at hand is inadequate, please ask me for further information. [Strong Rule] If you don't have any real-time information about the user’s query, please be honesty.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \" \" # ←- Insert your prompt here.\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Your System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_your_model = {\n",
    "    \"model\": \"solar-pro\",\n",
    "    \"max_tokens\": 0.0,  # ←- Insert your max_tokens: 0.0 ~ here. \n",
    "    \"temperature\": 0.0, # ←- Insert your temperature: 0.0 ~ 2.0 here.\n",
    "    \"top_p\": 0.0,       # ←- Insert your top_p: 0.0 ~ 1.0 here.\n",
    "}\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \" \" # ←- Insert your system prompt here.\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \" \" # ←- Insert your prompt here.\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {**config_your_model, \"messages\": message}\n",
    "\n",
    "response = client.chat.completions.create(**config)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
